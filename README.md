# Bucket Scanner - Cloud Storage Security Scanner

<p align="center">
  <img src="bucket_scanner_architecture.png" alt="Architecture Diagram" width="800"/>
</p>

A distributed, high-performance security scanner for detecting publicly exposed cloud storage buckets across AWS S3, Google Cloud Storage, and Azure Blob Storage.

## ğŸš€ Features

### Core Capabilities
- **Automated Enumeration**: Generate bucket names using patterns, wordlists, and permutations
- **Multi-Cloud Support**: Scan AWS S3, GCP Cloud Storage, and Azure Blob Storage
- **Public Access Detection**: Identify misconfigured buckets with proper pattern matching
- **Permission Analysis**: Check detailed bucket permissions and access levels
- **Content Analysis**: Discover sensitive files (credentials, keys, configs, etc.)
- **Rate Limiting**: Configurable rate limiting to avoid detection/blocking
- **IP Rotation**: Proxy pool support for distributed scanning
- **Real-time Notifications**: Slack/webhook alerts for critical findings

### Architecture Components
1. **Enumeration Layer**: Pattern-based and wordlist-based bucket name generation
2. **Input Layer**: Wordlist generation, name patterns, OSINT feeds
3. **Queue System**: Redis-based task distribution
4. **Scanner Workers**: DNS resolvers, HTTP probes, permission checkers
5. **Data Layer**: PostgreSQL for results, indexed with timeseries data
6. **REST API**: FastAPI-based API for integration
7. **Monitoring**: Prometheus + Grafana dashboards

## ğŸ“‹ Prerequisites

- Docker & Docker Compose
- Python 3.11+ (for local development)
- (Optional) Cloud provider credentials for authenticated scanning

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
git clone <repository-url>
cd task001

# Copy environment file
cp .env.example .env

# Edit .env with your configuration
nano .env
```

### 2. Start with Docker Compose

```bash
# From project root
docker compose up -d --build
```

This will start:
- **API Server** (port 8000)
- **PostgreSQL** (port 5432)
- **Redis** (port 6379)
- **Worker Service**(s) (2 replicas by default)
- **Prometheus** (port 9090)
- **Grafana** (port 3000)

### 3. Verify Installation

```bash
# Check API health
curl http://localhost:8000/health

# View logs
docker compose logs -f api
docker compose logs -f worker
```

### 4. Access Services

- API Documentation: http://localhost:8000/docs
- Grafana Dashboard: http://localhost:3000 (admin/admin)
- Prometheus: http://localhost:9090

## ğŸ“š Usage Examples

### Immediate Scan (Synchronous)

```bash
# Scan a bucket across all providers
curl -X POST http://localhost:8000/api/v1/scan/immediate \
  -H "Content-Type: application/json" \
  -d '{"bucket_name": "example-bucket"}'

# Scan specific provider
curl -X POST http://localhost:8000/api/v1/scan/immediate \
  -H "Content-Type: application/json" \
  -d '{"bucket_name": "example-bucket", "provider": "aws_s3"}'
```

### Queued Scan (Asynchronous)

```bash
# Queue a scan task
curl -X POST http://localhost:8000/api/v1/scan/queue \
  -H "Content-Type: application/json" \
  -d '{
    "bucket_name": "example-bucket",
    "priority": 10
  }'

# Check task status
curl http://localhost:8000/api/v1/task/{task_id}
```

### Bucket Name Enumeration

Generate potential bucket names automatically using patterns and wordlists:

```bash
# Generate bucket names for a company using patterns
curl -X POST http://localhost:8000/api/v1/enumerate \
  -H "Content-Type: application/json" \
  -d '{
    "company_name": "AcmeCorp",
    "max_names": 50,
    "use_wordlist": false,
    "auto_scan": false
  }'

# Generate names using wordlists
curl -X POST http://localhost:8000/api/v1/enumerate \
  -H "Content-Type: application/json" \
  -d '{
    "company_name": "Example",
    "max_names": 100,
    "use_wordlist": true,
    "wordlist_names": ["common", "aws-specific"],
    "auto_scan": false
  }'

# Generate and automatically queue for scanning
curl -X POST http://localhost:8000/api/v1/enumerate \
  -H "Content-Type: application/json" \
  -d '{
    "company_name": "Target",
    "max_names": 50,
    "use_wordlist": true,
    "auto_scan": true,
    "providers": ["aws_s3", "gcp_gcs"]
  }'

# List available wordlists
curl http://localhost:8000/api/v1/enumerate/wordlists

# Generate common public bucket patterns
curl -X POST http://localhost:8000/api/v1/enumerate/common-patterns \
  -H "Content-Type: application/json" \
  -d '{
    "limit": 100,
    "providers": ["aws_s3", "gcp_gcs", "azure_blob"],
    "auto_scan": true
  }'
```

**Enumeration Strategies:**
- **Pattern-based**: Generates names using common patterns (backup, data, uploads, etc.)
- **Environment-based**: Adds environment suffixes (prod, dev, staging, qa)
- **Purpose-based**: Combines purpose and environment (logs-prod, uploads-dev)
- **Date-based**: Includes year/month patterns (company-backup-2025)
- **Wordlist-based**: Uses curated wordlists for targeted scanning
- **Permutations**: Tests various separator combinations (-, _, .)

### Retrieve Results

```bash
# Get recent scan results
curl http://localhost:8000/api/v1/results?limit=10

# Get results for specific bucket
curl http://localhost:8000/api/v1/results/example-bucket

# Get all public buckets found
curl http://localhost:8000/api/v1/public-buckets

# Get security findings
curl http://localhost:8000/api/v1/findings?severity=critical
```

### Statistics

```bash
# Get overall statistics
curl http://localhost:8000/api/v1/statistics
```

## ğŸ”§ Configuration

### Environment Variables

Key configurations in `.env` (at project root):

```bash
# Rate Limiting
MAX_REQUESTS_PER_SECOND=10
MAX_CONCURRENT_WORKERS=50

# AWS Credentials (optional - for deeper scanning)
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret

# GCP Credentials
GCP_PROJECT_ID=your_project
GCP_CREDENTIALS_PATH=/path/to/credentials.json

# Azure Credentials
AZURE_ACCOUNT_NAME=your_account
AZURE_ACCOUNT_KEY=your_key

# Notifications
ENABLE_NOTIFICATIONS=true
SLACK_WEBHOOK=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
```

### Scaling Workers

```bash
# Edit .env at project root
WORKER_REPLICAS=5

# Restart services (from project root)
docker compose up -d --scale worker=5
```

## ğŸ—ï¸ Project Structure

```
task001/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scanner/          # Cloud scanner implementations
â”‚   â”‚   â”œâ”€â”€ aws_scanner.py
â”‚   â”‚   â”œâ”€â”€ gcp_scanner.py
â”‚   â”‚   â”œâ”€â”€ azure_scanner.py
â”‚   â”‚   â””â”€â”€ orchestrator.py
â”‚   â”œâ”€â”€ workers/          # Worker components
â”‚   â”‚   â”œâ”€â”€ dns_resolver.py
â”‚   â”‚   â”œâ”€â”€ http_probe.py
â”‚   â”‚   â”œâ”€â”€ permission_checker.py
â”‚   â”‚   â””â”€â”€ content_analyzer.py
â”‚   â”œâ”€â”€ enumeration/      # Bucket name enumeration
â”‚   â”‚   â”œâ”€â”€ name_generator.py
â”‚   â”‚   â””â”€â”€ wordlist_manager.py
â”‚   â”œâ”€â”€ queue/            # Queue system
â”‚   â”‚   â”œâ”€â”€ producer.py
â”‚   â”‚   â””â”€â”€ consumer.py
â”‚   â”œâ”€â”€ api/              # REST API
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ routes.py
â”‚   â”œâ”€â”€ database/         # Database models
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â””â”€â”€ repository.py
â”‚   â”œâ”€â”€ utils/            # Utilities
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py
â”‚   â”‚   â”œâ”€â”€ ip_rotator.py
â”‚   â”‚   â””â”€â”€ notifier.py
â”‚   â””â”€â”€ config/           # Configuration
â”‚       â””â”€â”€ settings.py
â”œâ”€â”€ wordlists/            # Enumeration wordlists
â”‚   â”œâ”€â”€ common.txt        # General patterns
â”‚   â””â”€â”€ aws-specific.txt  # AWS-specific patterns
â”œâ”€â”€ tests/                # Test suite
â”œâ”€â”€ docs/                 # Documentation
â”‚   â”œâ”€â”€ architecture_diagram.py
â”‚   â”œâ”€â”€ API.md
â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â””â”€â”€ DEVELOPMENT.md
â”œâ”€â”€ docker/               # Docker configuration
â”‚   â”œâ”€â”€ Dockerfile        # Container build definition
â”‚   â””â”€â”€ prometheus.yml    # Monitoring configuration
â”œâ”€â”€ docker-compose.yml    # Service orchestration (at root)
â”œâ”€â”€ .env.example          # Environment template
â””â”€â”€ requirements.txt
```

## ğŸ” How It Works

### Detection Patterns

#### AWS S3
1. Anonymous LIST operation test
2. Bucket ACL analysis (AllUsers, AuthenticatedUsers)
3. Bucket policy parsing for public statements
4. Public Access Block settings check

#### GCP Cloud Storage
1. Anonymous GET/LIST operations
2. IAM policy analysis (allUsers, allAuthenticatedUsers)
3. Bucket-level permission checks

#### Azure Blob Storage
1. Anonymous container listing
2. Container properties (Public Access Level)
3. Blob-level access tests

### Risk Levels

- **Critical**: Public write access or public bucket with sensitive data
- **High**: Public bucket with sensitive files (credentials, keys, etc.)
- **Medium**: Public read-only bucket with files
- **Low**: Private bucket or public but empty

## ğŸ§ª Testing

```bash
# Install dependencies
pip install -r requirements.txt

# Run tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=src --cov-report=html
```

## ğŸ“Š Monitoring

### Grafana Dashboards

Access Grafana at http://localhost:3000:
- Default credentials: admin/admin
- Import pre-built dashboards for:
  - Scan statistics
  - Queue metrics
  - Worker performance
  - Finding trends

### Prometheus Metrics

- API request rates
- Scan success/failure rates
- Queue depth
- Worker status

## ğŸ”’ Security Considerations

1. **Credentials**: Never commit credentials to version control
2. **Rate Limiting**: Respect cloud provider rate limits
3. **Legal**: Ensure you have permission to scan targets
4. **Network**: Use VPN/proxy for distributed operations
5. **Data**: Encrypt sensitive scan results at rest

## ğŸ› Troubleshooting

### Common Issues

**Services won't start:**
```bash
docker compose down -v
docker compose up -d
```

**Worker not processing tasks:**
```bash
docker compose logs worker
docker compose restart worker
```

**Database connection issues:**
```bash
docker compose restart postgres
# Wait for health check, then:
docker compose restart api worker
```

## ğŸ“– Documentation

- [API Documentation](docs/API.md) - Complete API reference
- [Deployment Guide](docs/DEPLOYMENT.md) - Production deployment
- [Development Guide](docs/DEVELOPMENT.md) - Local development setup

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## âš ï¸ Disclaimer

This tool is for security research and authorized testing only. Always obtain proper authorization before scanning systems you don't own. The authors are not responsible for misuse of this tool.

## ğŸ™ Acknowledgments

- Architecture inspired by distributed security scanning platforms
- Uses open-source libraries: FastAPI, SQLAlchemy, boto3, google-cloud-storage, azure-storage-blob

## ğŸ“ Support

For issues, questions, or contributions:
- Open an issue on GitHub
- Check existing documentation
- Review logs: `docker compose logs -f`
